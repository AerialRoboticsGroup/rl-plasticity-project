<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Maintaining Plasticity in Reinforcement Learning</title>
    <link rel="icon" type="image/svg" href="./static/UoB_Graphic_RGB_24.svg">

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <style>
        .tab-pane {
            display: none;
        }

        .tab-pane.active {
            display: block;
        }

        /* ÈöêËóèAbstract‰πãÂêéÁöÑÂÜÖÂÆπ */
        #hidden-content {
            display: none;
        }
    </style>

    <style>
      .logo-container {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 20px; /* Adjust spacing between logos */
      }
      /* Individual logo height adjustments */
      .logo-uob {
        height: 100px; /* Adjust as needed */
        width: auto;   /* Maintain aspect ratio */
      }
      .logo-brl {
        height: 100px;  /* Adjust as needed */
        width: auto;
      }
	.logo-neu {
        height: 100px; /* Adjust as needed */
        width: auto;   /* Maintain aspect ratio */
      }
    </style>
    

    <script>
        $(document).ready(function() {
            $(".tabs ul li a").click(function(e) {
                e.preventDefault();

                var targetTab = $(this).attr("href");

                $(".tabs ul li").removeClass("is-active");

                $(this).parent().addClass("is-active");

                $(".tab-content .tab-pane").removeClass("active");

                $(targetTab).addClass("active");
            });
        });
    </script>

</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Maintaining Plasticity in Reinforcement Learning: A Cost-Aware Framework for Aerial Robot Control in Non-stationary Environments</h1>
                        <h4 class="title is-4 conference-authors"><a target="_blank" href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros"> Submitted to IROS 2025</a></h4>

                        <!-- ‰ΩúËÄÖ -->
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                              <a target="_blank" href="https://tahirkarasahin.netlify.app/">Ali Tahir Karasahin</a><sup>1,2</sup>,</span>
                            <span class="author-block">
                              <a target="_blank" href="https://ziniuw.com/">Ziniu Wu</a><sup>2</sup>,</span>
                            <span class="author-block">
                              <a target="_blank" href="https://research-information.bris.ac.uk/en/persons/basaran-bahadir-kocer">Basaran Bahadir Kocer</a><sup>2,*</sup></span>
                        </div>

                        <!-- Âçï‰Ωç -->
                        <div class="is-size-5 publication-authors">
			    <span class="author-block"><sup>1</sup>Necmettin Erbakan University,</span>
                            <span class="author-block"><sup>2</sup>University of Bristol</span>
<!--                             <span class="author-block"><sup>3</sup>Bristol Robotics Laboratory</span> -->
                            <span class="author-block"><sup>*</sup>Corresponding author: Basaran Bahadir Kocer [b.kocer (at) bristol.ac.uk]</span>
                        </div>
			<br>
                        <!-- Logo -->
            			<div class="logo-container">
					<img class="logo-neu" src="./static/arg_logo-cropped.svg" alt="University of Bristol Logo">
<!--             				<img class="logo-uob" src="./static/UoB_RGB_24.png" alt="University of Bristol Logo"> -->
<!--             				<img class="logo-brl" src="./static/BRL_Logo_Main.png" alt="Bristol Flight Lab Logo"> -->
            			</div>
                        <br>
                         <!-- Buttons -->
                        <div class="column has-text-centered">
                            <div class="publication-links">
				    
            				<!-- PDF Link. -->
            				<span class="link-block">
            				<a href="https://arxiv.org/abs/123"
            				class="external-link button is-normal is-rounded is-dark">
            				<span class="icon">
            				<i class="fas fa-file-pdf"></i>
            				</span>
            				<span>Paper (coming soon)</span>
            				</a>
            				</span>
            				    
                                            <!-- ArXiv -->
            				<span class="link-block">
            				<a target="_blank" href="https://arxiv.org/pdf/123" class="external-link button is-normal is-rounded is-dark">
            				<span class="icon">
            				<i class="ai ai-arxiv"></i>
            				</span>
            				<span>ArXiv</span>
            				</a>
            				</span>
            				<!-- Video Link. -->
            				    
            				<span class="link-block">
            				<a href="https://www.youtube.com/watch?v=4"
            				class="external-link button is-normal is-rounded is-dark">
            				<span class="icon">
            				<i class="fab fa-youtube"></i>
            				</span>
            				<span>Video (coming soon)</span>
            				</a>
            				</span>
            				    
                                            <!-- Code Link. -->
            				<span class="link-block">
            				<a target="_blank" href="https://github.com/AerialRoboticsGroup/rl-plasticity"
            				class="external-link button is-normal is-rounded is-dark">
            				<span class="icon">
            				<i class="fab fa-github"></i>
            				</span>
            				<span>Code (coming soon)</span>
            				</a>
            				</span>

                                <!-- End of Buttons -->
                            </div>
                        </div>
                    </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

<!--     <div id="hidden-content" style="display: none;"> -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div class="item item-sweep_without_exceeding" style="display: flex; justify-content: center; align-items: center;">
                    <video poster="" id="sweep_without_exceeding" autoplay controls muted loop style="width:70%;" playbackRate=2.0>
                        <source src="./assets/images/123.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </section>
    </div>
    
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">üí°Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 110%">
                           Reinforcement learning (RL) has demonstrated the ability to maintain the plasticity of the policy throughout short-term training in aerial robot control. However, these policies have been shown to <span style="font-weight: bold">loss of plasticity</span> when extended to long-term learning in non-stationary environments. For example, the standard proximal policy optimization (PPO) policy is observed to collapse in long-term training settings and lead to significant control performance degradation. To address this problem, this work proposes a cost-aware framework that uses a <span style="font-weight: bold">retrospective cost mechanism (RECOM)</span> to balance rewards and losses in RL training with a <span style="font-weight: bold">non-stationary environment</span>. Using a cost gradient relation between rewards and losses, our framework dynamically updates the learning rate to continually train the control policy in a disturbed wind environment. Our experimental results show that our framework learned a policy for the hovering task without policy collapse and activated more dormant units in variable wind conditions.
                        </p>
			    <br>
			    <img src="./static/recom_teaser.png" alt="Your image description" style="display: block; margin-left: auto; margin-right: auto; width: auto;">
			    <p style="font-size: 80%">Illustration of aerial robot hovering from different initial positions under variable wind conditions</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!--Experiments-->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">What is the loss of plasticity?</span></h2>
<!--                         <span style="font-size: 110%">
                            coming soon
                        </span>
                        <br> -->
                        <br>
                        <img src="./static/whatislop.png" alt="Your image description" style="display: block; margin-left: auto; margin-right: auto; width: auto;">
			    <p style="font-size: 80%; text-align: center">During the 20M training, the wind disturbance value was changed every 2M timesteps to be [3.0, 2.0, 2.5, 1.5, 2.5] meter per second</p>
                        <br>
                        <span style="font-size: 110%">
                            RL has demonstrated the ability to maintain the plasticity throughout short-term training. However, these policies have been shown to loss of plasticity when extended to long-term learning in non-stationary environments. For example, We changed wind disturbance value every 2 million timesteps. The standard PPO policy is observed to collapse in long-term training settings and lead to significant control performance degradation.
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>


	    <!--recom-->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">RECOM Formulation</span></h2>
                        <span style="font-size: 110%">
                            To address this problem, this work proposes a cost-aware framework that uses a retrospective cost mechanism (RECOM) to balance rewards and losses in RL training with a non-stationary environment. Using a cost gradient relation between rewards and losses, our framework dynamically updates the learning rate to continually train the control policy in a disturbed wind environment.
                        </span>
                        <br>
                        <br>
                        <img src="./static/recom.png" alt="Your image description" style="display: block; margin-left: auto; margin-right: auto; width: auto;">
                        <br>
                    </div>
                </div>
            </div>
        </div>
    </section>
	
    <!--Framework-->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvima">Experiments</span></h2>
<!--                         <img src="./assets/images/alignbot-framework-new.png" alt="Your image description" style="display: block; margin-left: auto; margin-right: auto; width: auto;"> -->
                        <br>
				<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
				<tr>
				<th>Standard PPO</th>
				<th>L2 Regularization with PPO</th>
				<th>RECOM with L2 PPO</th>
				</tr>
				<tr>
				<td><img src="./static/eval_2.gif" width="100%"/></td>
				<td><img src="./static/eval_8.gif" width="100%"/></td>
				<td><img src="./static/eval_9.gif" width="100%"/></td>
				</tr>
				<tr>
				<td>Success Rate (%): 30</td>
				<td>Success Rate (%): 88</td>
				<td>Success Rate (%): 90</td>
				</tr>
				<br>
				</table>
				<br>
					<img src="./static/table3.png" alt="Your image description" style="display: block; margin-left: auto; margin-right: auto; width: 60%;">
					<p style="font-size: 80%; text-align: center">Simulation results: MSE error metrics comparisonfor three policies.</p>
				<br>
				<span style="font-size: 110%">
					<p>Our experimental results show that the baseline PPO policy collapses under dynamic wind changes, leading to significant control performance degradation. By integrating the proposed RECOM with the PPO algorithm, the policy remains stable throughout long-term training, and dormant units in the neural network are effectively utilized.</p>
				</span>
				<br>	
				<span style="font-size: 110%">üí°Key Findings:</span>
				<ul>
					<li>1. Policy collapse prevention during long-term training.</li>
					<li>2. Activation of dormant units in response to variable wind conditions.</li>
					<li>3. Demonstrated balance between rewards and losses through the RECOM.</li>
				</ul>
                    </div>
                </div>
            </div>
        </div>
    </section>




 
	
    <!--Conclusion-->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                
                <div class="rows is-centered ">
                    <div class="row is-full-width">          
                        <h2 class="title is-3"><span class="dvima">Conclusion</span></h2>
                        <br>
                        <span style="font-size: 110%">
                            In this work, we proposed a retrospective cost mechanism
(RECOM) to balance rewards and losses specifically focusing
on the aerial robot for hovering tasks in RotorPy with
non-stationary environment scenarios. With the proposed
RECOM with L2 PPO, policy collapse is prevented in
long-term training and the amount of dormant units in the
policy network is kept 11.29% at 20 million timesteps. The
experimental results demonstrated that rewards and losses
could be balanced with a retrospective mechanism inspired
by the perspective of neuroscience and cognitive science.
                        </span>
                        
                    </div>
                </div>
            </div>
        </div>
    </section>



	    <!--Acknowledgement-->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                
                <div class="rows is-centered ">
                    <div class="row is-full-width">          
                        <h2 class="title is-3"><span class="dvima">Acknowledgements</span></h2>
                        <br>
                        <span style="font-size: 110%">
				We would like to express our heartfelt gratitude to the researchers and developers behind the RotorPy simulation environment. Their work has been essential in providing a flexible and realistic platform for testing our reinforcement learning algorithms. We also thank the broader reinforcement learning and cognitive neuroscience communities for their valuable insights, which have greatly contributed to the development of our cost-aware RL framework.
                        We gratefully acknowledge the Bristol Robotics Laboratory for providing access to the flight arena. We also extend our appreciation to technician Patrick Brinson for his valuable assistance in conducting the experiments. 
				<!--Furthermore, we acknowledge Alex Dunnett for his contributions to photo editing and proofreading of this work.-->
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>


	
    <!--BibTeX-->
    <section class="section" id="BibTeX">
        <div class="container is-max-widescreen content">
            <h2 class="title">BibTeX (To be update)</h2>
            <pre><code>
@misc{ali_iros_rl_plasticity,
    title={Maintaining Plasticity in Reinforcement Learning: A Cost-Aware Framework for Aerial Robot Control in Non-stationary Environments}, 
    author={TBD},
    year={2025},
    eprint={24092342342345},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    url={https://arxiv.org/abs/23423}, 
}
            </code></pre>
        </div>
    </section>

<!--END OF WEBSITE-->

  

</div>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a herf="https://vimalabs.github.io/"> VIMA, <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>
